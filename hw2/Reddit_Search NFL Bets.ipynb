{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Licence\" data-toc-modified-id=\"Licence-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Licence</a></span></li><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Input\" data-toc-modified-id=\"Input-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Input</a></span></li><li><span><a href=\"#Output\" data-toc-modified-id=\"Output-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Output</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Functions-and-Classes\" data-toc-modified-id=\"Functions-and-Classes-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Functions and Classes</a></span></li><li><span><a href=\"#System-dependent-Configuration\" data-toc-modified-id=\"System-dependent-Configuration-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>System-dependent Configuration</a></span></li></ul></li><li><span><a href=\"#Collect-Data\" data-toc-modified-id=\"Collect-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Collect Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Collect-Reddit-Submissions-and-Comments\" data-toc-modified-id=\"Collect-Reddit-Submissions-and-Comments-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Collect Reddit Submissions and Comments</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licence\n",
    "-------------\n",
    "Developed by the Discovery Lab, Applied Intelligence Group, Accenture Federal Systems.\n",
    "\n",
    "```\n",
    "Copyright (c) 2020 Accenture Federal Systems.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "```\n",
    "\n",
    "\n",
    "Purpose\n",
    "-------------\n",
    "This notebook collects posts and comments, and associated metadata from the [Reddit](https://www.reddit.com/) social media platform. It uses Reddit API through [PRAW](https://praw.readthedocs.io/en/latest/) and requires credentials.\n",
    "\n",
    "\n",
    "Input\n",
    "-------------\n",
    "**Required Parameters**\n",
    "\n",
    "- _**client\\_id**_ (string) - A unique client id provided by Reddit.\n",
    "\n",
    "- _**client\\_secret**_ (string) - Secret associated with client id provided by Reddit.\n",
    "\n",
    "- _**user\\_agent**_ (string) - A unique user agent provided by Reddit.\n",
    "\n",
    "- _**search\\_terms**_ (array of strings) - Search terms.\n",
    "\n",
    "- _**subreddits**_ (array of strings) - Names of subreddits to search from.\n",
    "\n",
    "\n",
    "**Optional Parameters**\n",
    "\n",
    "- _**post\\_limit**_ (integer, default: 100, maximum: 1000) - Maximum number of posts to download.\n",
    "\n",
    "\n",
    "Output\n",
    "-------------\n",
    "The outputs are two `CSV` files named `REDDIT_POSTS_{{DATETIME}}.csv` and `REDDIT_COMMENTS_{{DATETIME}}.csv`, where `{{DATETIME}}` is the approximate date/time of the data collection. These `CSV` files are saved in the `{{HOME}}/data` folder, where `{{HOME}}` is the project installation folder.\n",
    "\n",
    "The columns of the output file `REDDIT_POSTS_{{DATETIME}}.csv` is as follows.\n",
    "\n",
    "- _**author**_ (string) -  Provides an instance of `Redditor`.\n",
    "\n",
    "- _**clicked**_ (binary) -  Whether or not the submission has been clicked by the client. \n",
    "\n",
    "- _**comments**_ (array of strings) -  Provides an instance of `CommentForest`. \n",
    "\n",
    "- _**created_utc**_ (datetime) - Time the submission was created, represented in Unix Time. \n",
    "\n",
    "- _**distinguished**_ (binary) - Whether or not the submission is distinguished. \n",
    "\n",
    "- _**edited**_ (binary) - Whether or not the submission has been edited. \n",
    "\n",
    "- _**id**_ (string) - ID of the submission. \n",
    "\n",
    "- _**is\\_original_content**_ (binary) - Whether or not the submission has been set as original content. \n",
    "\n",
    "- _**is\\_self**_ (binary) - Whether or not the submission is a selfpost (text-only). \n",
    "\n",
    "- _**link\\_flair_template\\_id**_ (string) - The link flair’s ID, or None if not flaired. \n",
    "\n",
    "- _**link\\_flair\\_text**_ (text) - The link flair’s text content, or None if not flaired. \n",
    "\n",
    "- _**locked**_ (binary) - Whether or not the submission has been locked. \n",
    "\n",
    "- _**name**_ (string) - Fullname of the submission. \n",
    "\n",
    "- _**num\\_comments**_ (integer) - The number of comments on the submission. \n",
    "\n",
    "- _**over\\_18**_ (binary) - Whether or not the submission has been marked as NSFW. \n",
    "\n",
    "- _**permalink**_ (string) - A permalink for the submission. \n",
    "\n",
    "- _**poll\\_data**_ (object) - A PollData object representing the data of this submission, if it is a poll submission. \n",
    "\n",
    "- _**score**_ (integer) - The number of upvotes for the submission. \n",
    "\n",
    "- _**selftext**_ (text) - The submissions’ selftext - an empty string if a link post. \n",
    "\n",
    "- _**spoiler**_ (binary) - Whether or not the submission has been marked as a spoiler. \n",
    "\n",
    "- _**stickied**_ (binary) - Whether or not the submission is stickied. \n",
    "\n",
    "- _**subreddit**_ (string) - Provides an instance of Subreddit. \n",
    "\n",
    "- _**title**_ (text) - The title of the submission. \n",
    "\n",
    "- _**upvote\\_ratio**_ (double) - The percentage of upvotes from all votes on the submission. \n",
    "\n",
    "- _**url**_ (string) - The URL the submission links to, or the permalink if a selfpost. \n",
    "\n",
    "\n",
    "The columns of the output file `REDDIT_COMMENTS_{{DATETIME}}.csv` is as follows.\n",
    "\n",
    "- _**author**_ (string) - Provides an instance of Redditor. \n",
    "\n",
    "- _**body**_ (text) -  The body of the comment, as Markdown.\n",
    "\n",
    "- _**body\\_html**_ (text) - The body of the comment, as HTML.\n",
    "\n",
    "- _**created\\_utc**_ (datetime) - Time the comment was created, represented in Unix Time. \n",
    "\n",
    "- _**distinguished**_ (binary) - Whether or not the comment is distinguished. \n",
    "\n",
    "- _**edited**_ (binary) - Whether or not the comment has been edited. \n",
    "\n",
    "- _**id**_ (string) - ID of the comment. \n",
    "\n",
    "- _**is\\_submitter**_ (binary) - Whether or not the comment author is also the author of the submission. \n",
    "\n",
    "- _**link\\_id**_ (string) - The submission ID that the comment belongs to. \n",
    "\n",
    "- _**parent\\_id**_ (string) - The ID of the parent comment (prefixed with t1\\_). If it is a top-level comment, this returns the submission ID instead (prefixed with t3\\_). \n",
    "\n",
    "- _**permalink**_ (string) - A permalink for the comment. Comment objects from the inbox have a context attribute instead. \n",
    "\n",
    "- _**replies**_ (integer) - Provides an instance of CommentForest. \n",
    "\n",
    "- _**score**_ (integer) - The number of upvotes for the comment. \n",
    "\n",
    "- _**stickied**_ (binary) - Whether or not the comment is stickied. \n",
    " \n",
    "- _**submission**_ (string) - Provides an instance of the submission that the comment belongs to. \n",
    "\n",
    "- _**subreddit**_ (string) - Provides an instance of the subreddit that the comment belongs to. \n",
    "\n",
    "- _**subreddit\\_id**_ (string) - The subreddit ID that the comment belongs to. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The imports, function and class defintions, global variables, and system-dependent configuration are in this section. </p>\n",
    "\n",
    "<p> The system dependent configuration should be carefully reviewed and configured for each system (e.g., Linux vs. Windows, or the path of an external program) since the playbook will most likely fail without proper configuration. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (4.4.3)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: sortedcontainers in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Requirement already satisfied: praw in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (7.6.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from prawcore<3,>=2.1->praw) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3)\n",
      "Requirement already satisfied: six in /Users/pranavbhadharla/opt/anaconda3/lib/python3.9/site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# installing some of the packages for later \n",
    "!pip install selenium\n",
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q1/k8xy5vbn4mvb32fd180tdqk80000gn/T/ipykernel_22553/4234844595.py:14: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('retina')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"This cell imports necessary Python modules and performs initial configuration\n",
    "\"\"\"\n",
    "\n",
    "# Data manipulation libraries\n",
    "import json\n",
    "import pandas as pd \n",
    "import csv\n",
    "\n",
    "\n",
    "# Visualization and Interaction\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "from IPython.display import set_matplotlib_formats, display, clear_output, HTML\n",
    "set_matplotlib_formats('retina')\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \n",
    "init_notebook_mode(connected=True)\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipywidgets import VBox, HBox, Button, HTML, Label\n",
    "\n",
    "\n",
    "# Computation libraries \n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "# Graph analysis\n",
    "# import networkx as nx\n",
    "# import community\n",
    "\n",
    "\n",
    "# System related\n",
    "# import sys\n",
    "# import warnings;\n",
    "# warnings.filterwarnings('ignore')\n",
    "import io\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import os\n",
    "from getpass import getpass\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# Datetime libraries\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pytz import timezone\n",
    "\n",
    "\n",
    "# NLP dependencies\n",
    "# import spacy\n",
    "# from spacy.tokenizer import Tokenizer\n",
    "# nlp = spacy.load('en')\n",
    "# tokenizer = Tokenizer(nlp.vocab)\n",
    "# from langdetect import detect\n",
    "\n",
    "\n",
    "# Scraping libraries\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Machine learning libraries\n",
    "# from sklearn import datasets\n",
    "# from sklearn import linear_model\n",
    "# from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Logging\n",
    "import logging \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# For Reddit\n",
    "import praw\n",
    "from praw.models import MoreComments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell defines global variables and parameters used throughout the playbook\n",
    "\"\"\"\n",
    "\n",
    "# Set this to True if you want to watch Selenium scrape pages\n",
    "# WATCH_SCRAPING = True\n",
    "\n",
    "# Set this to True if you want to use incognito mode\n",
    "# USE_INCOGNITO = True\n",
    "\n",
    "# increased number of posts for more data\n",
    "\n",
    "# Number of posts \n",
    "post_limit = 1000\n",
    "\n",
    "# changed storage location \n",
    "\n",
    "# The data is written \n",
    "RAW_DATA_DIRECTORY = Path(\"../data\")\n",
    "\n",
    "# Setup logging level\n",
    "LOGGING_LEVEL = logging.INFO \n",
    "logging.basicConfig(level=LOGGING_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell defines functions and classes used throughout the playbook\n",
    "\"\"\"\n",
    "\n",
    "def __init__(self, client_id, client_
    , user_agent, password):\n",
    "    self.client_id = client_id\n",
    "    self.client_secret = client_secret\n",
    "    self.user_agent = user_agent\n",
    "    self.password = password\n",
    "\n",
    "\n",
    "def token(client_id, client_secret, user_agent):\n",
    "    reddit = praw.Reddit(client_id=client_id,\n",
    "                         client_secret=client_secret,\n",
    "                         user_agent=user_agent)\n",
    "    if (reddit != False):\n",
    "        print(\"Successful token\")\n",
    "    else:\n",
    "        print(\"Failed token\")\n",
    "    return reddit\n",
    "\n",
    "\n",
    "def search_reddit(reddit, search_term, sort_type, time_limit, post_limit):\n",
    "    \"\"\"\n",
    "    GRAB REDDIT POSTS BY SEARCH TERM\n",
    "    search_term = any boolean search #https://www.reddit.com/dev/api/\n",
    "    sort_type = 'relevance', 'hot', 'top', 'new', 'comments'\n",
    "    time_limit = 'all', 'hour', 'day', 'week', 'month', 'year'\n",
    "    post_limit = 1000 maximum\n",
    "    \"\"\"\n",
    "\n",
    "    posts = []\n",
    "    subreddit = reddit.subreddit(\"all\")\n",
    "    for post in subreddit.search(search_term, sort=sort_type, time_filter=time_limit, limit=post_limit):\n",
    "        posts.append([post.subreddit, post.id, post.title, post.selftext, post.author, post.url, post.permalink,\n",
    "                      post.num_comments, post.created, post.score, post.distinguished, post.is_original_content,\n",
    "                      post.upvote_ratio, post.link_flair_text])\n",
    "    posts = pd.DataFrame(posts,\n",
    "                         columns=['subreddit', 'post_id', 'title', 'post_body', 'post_author', 'url', 'post_permalink',\n",
    "                                  'num_comments', 'post_created', 'post_score', 'post_distinguished',\n",
    "                                  'original_content', 'upvote_ratio', 'flair_text'])\n",
    "    posts['post_created'] = pd.to_datetime(posts['post_created'], unit='s')\n",
    "    posts['scrape_time'] = datetime.now()\n",
    "    posts[['subreddit', 'post_id', 'title', 'post_author',\n",
    "           'post_body', 'url', 'post_permalink', 'flair_text']] = posts[['subreddit', 'post_id', 'title', 'post_author',\n",
    "                                                                         'post_body', 'url', 'post_permalink',\n",
    "                                                                         'flair_text']].astype(str)\n",
    "    return posts\n",
    "\n",
    "\n",
    "def get_subreddit(reddit, sub, sort_type, time_limit, post_limit):\n",
    "    '''\n",
    "    GRAB REDDIT POSTS BY SUBREDDIT\n",
    "    sub = subreddits\n",
    "    sort_type = 'hot', 'top', 'new', 'gilded', 'rising', 'controversial'\n",
    "    time_limit = 'all', 'hour', 'day', 'week', 'month', 'year'\n",
    "    post_limit = 1000 maximum\n",
    "    '''\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    posts = []\n",
    "    if (sort_type == \"top\") or (sort_type == \"hot\") or (sort_type == \"controversial\"):\n",
    "        for post in subreddit.top(time_filter=time_limit, limit=post_limit):\n",
    "            posts.append([post.subreddit, post.id, post.title, post.selftext, post.author, post.url, post.permalink,\n",
    "                          post.num_comments, post.created, post.score, post.distinguished, post.is_original_content,\n",
    "                          post.upvote_ratio, post.link_flair_text])\n",
    "\n",
    "    if (sort_type == \"new\") or (sort_type == \"rising\"):\n",
    "        for post in subreddit.new(limit=post_limit):\n",
    "            posts.append([post.subreddit, post.id, post.title, post.selftext, post.author, post.url, post.permalink,\n",
    "                          post.num_comments, post.created, post.score, post.distinguished, post.is_original_content,\n",
    "                          post.upvote_ratio, post.link_flair_text])\n",
    "\n",
    "    posts = pd.DataFrame(posts,\n",
    "                         columns=['subreddit', 'post_id', 'title', 'post_body', 'post_author', 'url', 'post_permalink',\n",
    "                                  'num_comments', 'post_created', 'post_score', 'post_distinguished',\n",
    "                                  'original_content', 'upvote_ratio', 'flair_text'])\n",
    "    posts['post_created'] = pd.to_datetime(posts['post_created'], unit='s')\n",
    "    posts['scrape_time'] = datetime.now()\n",
    "    posts[['subreddit', 'post_id', 'title', 'post_author',\n",
    "           'post_body', 'url', 'post_permalink', 'flair_text', ]] = posts[\n",
    "        ['subreddit', 'post_id', 'title', 'post_author',\n",
    "         'post_body', 'url', 'post_permalink', 'flair_text']].astype(str)\n",
    "    return posts\n",
    "\n",
    "\n",
    "def get_reddit_comments(reddit, post_id):\n",
    "    submission = reddit.submission(id=post_id)\n",
    "    comment = []\n",
    "    for top_level in submission.comments:\n",
    "        if isinstance(top_level, MoreComments):\n",
    "            continue\n",
    "        comment.append([top_level.subreddit, top_level.submission, top_level.id, top_level.parent_id, top_level.author,\n",
    "                        top_level.permalink, top_level.body, top_level.created, top_level.score,\n",
    "                        top_level.distinguished])\n",
    "    comments = pd.DataFrame(comment, columns=['subreddit', 'post_id', 'comment_id', 'parent_id', 'comment_author',\n",
    "                                              'comment_permalink', 'comment_body', 'comment_created', 'comment_score',\n",
    "                                              'comment_distinguished'])\n",
    "    comments['comment_created'] = pd.to_datetime(comments['comment_created'], unit='s')\n",
    "    comments['scrape_time'] = datetime.now()\n",
    "    comments[['subreddit', 'post_id', 'comment_id', 'parent_id',\n",
    "              'comment_author', 'comment_permalink', 'comment_body']] = comments[\n",
    "        ['subreddit', 'post_id', 'comment_id', 'parent_id',\n",
    "         'comment_author', 'comment_permalink', 'comment_body']].astype(str)\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System-dependent Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell defines system-dependent configuration such as those different in Linux vs. Windows\n",
    "\"\"\"\n",
    "\n",
    "# Get the system information from the OS\n",
    "PLATFORM_SYSTEM = platform.system()\n",
    "\n",
    "# Darwin is macOS\n",
    "if PLATFORM_SYSTEM == \"Darwin\":\n",
    "    EXECUTABLE_PATH = Path(\"../dependencies/chromedriver\")\n",
    "elif PLATFORM_SYSTEM == \"Windows\":\n",
    "    EXECUTABLE_PATH = Path(\"../dependencies/chromedriver.exe\")\n",
    "else:\n",
    "    logging.critical(\"Chromedriver not found or Chromedriver is outdated...\")\n",
    "    exit()\n",
    "    \n",
    "tz = timezone('US/Eastern')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Reddit Submissions and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter client_id: ········\n",
      "Enter client secret: ········\n",
      "Enter user agent: collectPosts\n",
      "Enter search terms (seperated by spaces): nfl+bets\n",
      "Enter subreddits (separated by spaces): sportsbook\n",
      "Successful token\n",
      "Grabbed 247 posts with search term: nfl+bets\n",
      "Grabbed 851 posts from subreddit: sportsbook\n",
      "Number of posts:  1098\n",
      "Exported posts to CSV\n",
      "Number of total comments:  24082\n",
      "Exported comments to CSV\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This cell retrieves page posts and comments, for a given page.\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    # Credentials (create client_id, client_secret, user_agent by following https://praw.readthedocs.io/en/latest/getting_started/quick_start.html)\n",
    "    client_id = getpass(\"Enter client_id: \")\n",
    "    client_secret = getpass(\"Enter client secret: \")\n",
    "    user_agent = input(\"Enter user agent: \")\n",
    "\n",
    "    ''' Designate input parameters for functions\n",
    "    search terms = key terms to search ALL of reddit\n",
    "    subreddits = subreddits to collect\n",
    "    search_sort_type = 'relevance', 'hot', 'top', 'new', 'comments'\n",
    "    sub_sort_type = 'hot', 'top', 'new', 'gilded', 'rising', 'controversial'\n",
    "    time_limit = 'all', 'hour', 'day', 'week', 'month', 'year'\n",
    "    post_limit = 10\n",
    "    '''\n",
    "\n",
    "    sub_sort_type = 'new'  # , 'top', 'new', 'gilded', 'rising', 'controversial'\n",
    "    search_sort_type = 'new'  # , 'hot', 'top', 'new', 'comments', 'relevance'\n",
    "    time_limit = 'year'  # , 'hour', 'day', 'week', 'month', 'year'\n",
    "      # maximum of 1000\n",
    "\n",
    "    \n",
    "    # search_terms = [\"covid19\", \"coronavirus\"]\n",
    "    # subreddits = [\"CoronavirusMemes\", \"Coronavirus\", \"CoronavirusUS\"]\n",
    "\n",
    "    input_search_terms = input(\"Enter search terms (seperated by spaces): \")\n",
    "    search_terms = input_search_terms.split()\n",
    "    \n",
    "    input_subreddits = input(\"Enter subreddits (separated by spaces): \")\n",
    "    subreddits = input_subreddits.split()\n",
    "    \n",
    "    ''' Collect posts & corresponding comments\n",
    "    '''\n",
    "    # Create client\n",
    "    r = token(client_id, client_secret, user_agent)\n",
    "    list_posts_df = []\n",
    "    try:\n",
    "        for query in search_terms:\n",
    "            post_df = search_reddit(r, query, search_sort_type, time_limit, post_limit)\n",
    "            print(\"Grabbed\", len(post_df), \"posts with search term:\", query)\n",
    "            list_posts_df.append(post_df)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for sub in subreddits:\n",
    "            post_df = get_subreddit(r, sub, sub_sort_type, time_limit, post_limit)\n",
    "            print(\"Grabbed\", len(post_df), \"posts from subreddit:\", sub)\n",
    "            list_posts_df.append(post_df)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    new_posts = pd.concat(list_posts_df)\n",
    "    print(\"Number of posts: \", new_posts.shape[0])\n",
    "\n",
    "    \n",
    "#     slightly tweaked storage locations \n",
    "\n",
    "    # File output for posts\n",
    "    file_name_csv = \"REDDIT_POSTS_\" + datetime.now(tz=tz).strftime(\"%Y-%m-%dT%H-%M-%S%z\") + \".csv\"\n",
    "    new_posts.to_csv(str(RAW_DATA_DIRECTORY / file_name_csv), index=False)\n",
    "    print(\"Exported posts to CSV\")\n",
    "\n",
    "    # Get comments\n",
    "    post_ids = new_posts['post_id'].to_list()\n",
    "    list_comment_dfs = []\n",
    "    i = 0\n",
    "    for post in post_ids:\n",
    "        try:\n",
    "            comment_df = get_reddit_comments(r, post)\n",
    "            list_comment_dfs.append(comment_df)\n",
    "            # print(i, \"Grabbed\", len(comment_df), \"comments from post id:\", post)\n",
    "        except:\n",
    "            pass\n",
    "        i += 1\n",
    "    post_comments = pd.concat(list_comment_dfs)\n",
    "    print(\"Number of total comments: \", post_comments.shape[0])\n",
    "\n",
    "    # File output for comment\n",
    "    com_filename_csv = \"REDDIT_COMMENTS_\" + datetime.now(tz=tz).strftime(\"%Y-%m-%dT%H-%M-%S%z\") + \".csv\"\n",
    "    post_comments.to_csv(str(RAW_DATA_DIRECTORY / com_filename_csv), index=False)\n",
    "    print(\"Exported comments to CSV\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "# client_id = '",
    "# client_secret = '",
    "# user_agent = 'collectPosts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Add post-processing steps here\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Clean up the environment\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Add post-processing steps here\n",
    "\"\"\"\n",
    "\n",
    "# Clean up the environment\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
